{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "class BackgroundModel:\n",
    "    def __init__(self, width, height, num_clusters=5, manhattan_threshold=10, L=512):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_clusters = num_clusters\n",
    "        self.manhattan_threshold = manhattan_threshold\n",
    "        self.L = L\n",
    "\n",
    "        # Initialize clusters: (weight, Y, Cb, Cr)\n",
    "        self.clusters = np.zeros((height, width, num_clusters, 4))  # (weight, Y, Cb, Cr)\n",
    "\n",
    "    def manhattan_distance(self, centroid, pixel):\n",
    "        \"\"\"Compute Manhattan distance between a centroid and a pixel.\"\"\"\n",
    "        Y, Cb, Cr = centroid[1:]  # Centroid's components\n",
    "        p_Y, p_Cb, p_Cr = pixel   # Pixel components\n",
    "        return abs(Y - p_Y) + abs(Cb - p_Cb) + abs(Cr - p_Cr)\n",
    "\n",
    "    def update_weights(self, weights, matched_index):\n",
    "        \"\"\"Update weights.\"\"\"\n",
    "        for k in range(len(weights)):\n",
    "            if k == matched_index:\n",
    "                weights[k] += (1 / self.L) * (1 - weights[k])\n",
    "            else:\n",
    "                weights[k] += (1 / self.L) * (0 - weights[k])\n",
    "\n",
    "    def normalize_weights(self, weights):\n",
    "        \"\"\"Normalize weights to sum to 1.\"\"\"\n",
    "        total = np.sum(weights)\n",
    "        return weights / total if total > 0 else weights\n",
    "\n",
    "    def adapt_centroid(self, centroid, pixel):\n",
    "        \"\"\"Adapt the centroid towards the incoming pixel.\"\"\"\n",
    "        error = centroid[1:] - pixel\n",
    "        overflow = error > self.L - 1\n",
    "        underflow = error < -self.L\n",
    "        adjustment = np.where(overflow, -1, np.where(underflow, 1, 0))\n",
    "        centroid[1:] += adjustment\n",
    "\n",
    "    def classify_pixel(self, cluster_weights, matched_index):\n",
    "        \"\"\"Classify a pixel as foreground or background.\"\"\"\n",
    "        P = np.sum(cluster_weights[matched_index + 1:])\n",
    "        return P\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame.\"\"\"\n",
    "        output = np.zeros((self.height, self.width), dtype=np.uint8)\n",
    "\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                pixel = frame[y, x]\n",
    "                cluster_group = self.clusters[y, x]\n",
    "\n",
    "                # Step 1: Cluster matching\n",
    "                distances = [\n",
    "                    self.manhattan_distance(cluster, pixel)\n",
    "                    for cluster in cluster_group\n",
    "                ]\n",
    "                matches = [i for i, d in enumerate(distances) if d <= self.manhattan_threshold]\n",
    "\n",
    "                if matches:\n",
    "                    # Matching cluster found\n",
    "                    matched_index = matches[0]\n",
    "                    self.adapt_centroid(cluster_group[matched_index], pixel)\n",
    "                    self.update_weights(cluster_group[:, 0], matched_index)\n",
    "                else:\n",
    "                    # No matching cluster found\n",
    "                    min_weight_index = np.argmin(cluster_group[:, 0])\n",
    "                    cluster_group[min_weight_index] = np.array([0.01, *pixel])\n",
    "\n",
    "                # Normalize weights\n",
    "                cluster_group[:, 0] = self.normalize_weights(cluster_group[:, 0])\n",
    "\n",
    "                # Sort clusters by weight\n",
    "                cluster_group = cluster_group[np.argsort(cluster_group[:, 0])[::-1]]\n",
    "\n",
    "                # Classification\n",
    "                P = self.classify_pixel(cluster_group[:, 0], matches[0] if matches else -1)\n",
    "                output[y, x] = 255 if P > 0.5 else 0  # Binary foreground/background\n",
    "\n",
    "                # Save back sorted clusters\n",
    "                self.clusters[y, x] = cluster_group\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved foreground frame: foreground_frames1\\foreground_0000.png\n",
      "Saved foreground frame: foreground_frames1\\foreground_0002.png\n",
      "Saved foreground frame: foreground_frames1\\foreground_0004.png\n",
      "Saved foreground frame: foreground_frames1\\foreground_0006.png\n",
      "Saved foreground frame: foreground_frames1\\foreground_0008.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Foreground frames saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m frame_ycbcr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame_resized, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2YCrCb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Process frame\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m foreground \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_ycbcr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Save the foreground image to a file\u001b[39;00m\n\u001b[0;32m     43\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeground_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 79\u001b[0m, in \u001b[0;36mBackgroundModel.process_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     76\u001b[0m cluster_group \u001b[38;5;241m=\u001b[39m cluster_group[np\u001b[38;5;241m.\u001b[39margsort(cluster_group[:, \u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_pixel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_group\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m output[y, x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m P \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Binary foreground/background\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Save back sorted clusters\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m, in \u001b[0;36mBackgroundModel.classify_pixel\u001b[1;34m(self, cluster_weights, matched_index)\u001b[0m\n\u001b[0;32m     38\u001b[0m     adjustment \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(overflow, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mwhere(underflow, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     39\u001b[0m     centroid[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m adjustment\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_pixel\u001b[39m(\u001b[38;5;28mself\u001b[39m, cluster_weights, matched_index):\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Classify a pixel as foreground or background.\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     P \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(cluster_weights[matched_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "def main():\n",
    "    # Open a video file or capture from a camera\n",
    "    cap = cv2.VideoCapture(r'C:\\Users\\muxia\\Desktop\\ece420final\\sample4.mp4')  # Replace 'video.mp4' with 0 for live camera\n",
    "\n",
    "    # Check if video capture is successful\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read video\")\n",
    "        exit()\n",
    "\n",
    "    # Define target dimensions\n",
    "    target_width = 320\n",
    "    target_height = 240\n",
    "\n",
    "    # Initialize BackgroundModel with resized frame dimensions\n",
    "    model = BackgroundModel(target_width, target_height)\n",
    "\n",
    "    # Create a directory to store the foreground images\n",
    "    output_dir = \"foreground_frames1\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    frame_count = 0  # To track the frame number\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if(frame_count%2 != 0):\n",
    "            frame_count += 1\n",
    "            continue\n",
    "        # Resize the frame to 320x240\n",
    "\n",
    "        frame_resized = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "        # Convert the resized frame to YCbCr format\n",
    "        frame_ycbcr = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "        # Process frame\n",
    "        foreground = model.process_frame(frame_ycbcr)\n",
    "\n",
    "        # Save the foreground image to a file\n",
    "        output_path = os.path.join(output_dir, f\"foreground_{frame_count:04d}.png\")\n",
    "        cv2.imwrite(output_path, foreground)\n",
    "\n",
    "        print(f\"Saved foreground frame: {output_path}\")\n",
    "        frame_count += 1\n",
    "\n",
    "        # Break on 'q' key press (optional if running interactively)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    print(\"Processing complete. Foreground frames saved.\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class BackgroundModel:\n",
    "    def __init__(self, width, height, num_clusters=5, manhattan_threshold=5, L=512):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_clusters = num_clusters\n",
    "        self.manhattan_threshold = manhattan_threshold\n",
    "        self.L = L\n",
    "        self.clusters = np.zeros((height, width, num_clusters, 4))  # (weight, Y, Cb, Cr)\n",
    "\n",
    "    def manhattan_distance(self, centroid, pixel):\n",
    "        Y, Cb, Cr = centroid[1:]\n",
    "        p_Y, p_Cb, p_Cr = pixel\n",
    "        return abs(Y - p_Y) + abs(Cb - p_Cb) + abs(Cr - p_Cr)\n",
    "\n",
    "    def update_weights(self, weights, matched_index):\n",
    "        for k in range(len(weights)):\n",
    "            if k == matched_index:\n",
    "                weights[k] += (1 / self.L) * (1 - weights[k])\n",
    "            else:\n",
    "                weights[k] += (1 / self.L) * (0 - weights[k])\n",
    "\n",
    "    def normalize_weights(self, weights):\n",
    "        total = np.sum(weights)\n",
    "        return weights / total if total > 0 else weights\n",
    "\n",
    "    def adapt_centroid(self, centroid, pixel):\n",
    "        error = centroid[1:] - pixel\n",
    "        overflow = error > self.L - 1\n",
    "        underflow = error < -self.L\n",
    "        adjustment = np.where(overflow, -1, np.where(underflow, 1, 0))\n",
    "        centroid[1:] += adjustment\n",
    "\n",
    "    def classify_pixel(self, cluster_weights, matched_index):\n",
    "        P = np.sum(cluster_weights[matched_index + 1:])\n",
    "        return P\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        output = np.zeros((self.height, self.width), dtype=np.uint8)\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                pixel = frame[y, x]\n",
    "                cluster_group = self.clusters[y, x]\n",
    "                distances = [self.manhattan_distance(cluster, pixel) for cluster in cluster_group]\n",
    "                matches = [i for i, d in enumerate(distances) if d <= self.manhattan_threshold]\n",
    "                if matches:\n",
    "                    matched_index = matches[0]\n",
    "                    self.adapt_centroid(cluster_group[matched_index], pixel)\n",
    "                    self.update_weights(cluster_group[:, 0], matched_index)\n",
    "                else:\n",
    "                    min_weight_index = np.argmin(cluster_group[:, 0])\n",
    "                    cluster_group[min_weight_index] = np.array([0.01, *pixel])\n",
    "                cluster_group[:, 0] = self.normalize_weights(cluster_group[:, 0])\n",
    "                cluster_group = cluster_group[np.argsort(cluster_group[:, 0])[::-1]]\n",
    "                P = self.classify_pixel(cluster_group[:, 0], matches[0] if matches else -1)\n",
    "                output[y, x] = 255 if P > 0.5 else 0\n",
    "                self.clusters[y, x] = cluster_group\n",
    "        return output\n",
    "\n",
    "def postprocess_foreground(foreground, area_threshold=10):\n",
    "    \"\"\"Apply post-processing to remove false positives and fill holes.\"\"\"\n",
    "    # Morphological opening to remove small noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    clean_foreground = cv2.morphologyEx(foreground, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Connected components to eliminate small regions\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(clean_foreground, connectivity=8)\n",
    "    for i in range(1, num_labels):  # Skip the background (label 0)\n",
    "        if stats[i, cv2.CC_STAT_AREA] < area_threshold:\n",
    "            clean_foreground[labels == i] = 0\n",
    "\n",
    "    # Fill holes in the remaining regions\n",
    "    filled_foreground = cv2.morphologyEx(clean_foreground, cv2.MORPH_CLOSE, kernel)\n",
    "    return filled_foreground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved foreground frame: foreground_frames\\foreground_0000.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0005.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0010.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0015.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0020.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0025.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0030.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0035.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0040.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0045.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0050.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0055.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0060.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0065.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0070.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0075.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0080.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0085.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0090.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0095.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0100.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0105.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0110.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0115.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0120.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0125.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0130.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0135.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0140.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0145.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0150.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0155.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0160.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0165.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0170.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0175.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0180.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0185.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0190.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0195.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0200.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0205.png\n",
      "Saved foreground frame: foreground_frames\\foreground_0210.png\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(r'C:\\Users\\muxia\\Desktop\\ece420final\\sample4.mp4')\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    exit()\n",
    "\n",
    "target_width = 320\n",
    "target_height = 240\n",
    "model = BackgroundModel(target_width, target_height)\n",
    "\n",
    "output_dir = \"foreground_frames\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if frame_count % 5 != 0:\n",
    "        frame_count += 1\n",
    "        continue\n",
    "    frame_resized = cv2.resize(frame, (target_width, target_height))\n",
    "    frame_ycbcr = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2YCrCb)\n",
    "    foreground = model.process_frame(frame_ycbcr)\n",
    "\n",
    "    # Apply postprocessing\n",
    "    processed_foreground = postprocess_foreground(foreground)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"foreground_{frame_count:04d}.png\")\n",
    "    cv2.imwrite(output_path, processed_foreground)\n",
    "    print(f\"Saved foreground frame: {output_path}\")\n",
    "    frame_count += 1\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "print(\"Processing complete. Foreground frames saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
